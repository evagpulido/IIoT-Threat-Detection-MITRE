{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f241ad9a-ec72-4982-8625-7b56f37a3945",
   "metadata": {},
   "source": [
    "# Preprocesado y limpieza del NG-IIoTset para aplicaciones de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecab9a0e-62b2-4636-8ffa-453de2ca4d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import ipaddress\n",
    "import datetime\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6b02b-ce6e-41b6-8f7b-540b411d0b2d",
   "metadata": {},
   "source": [
    "### Carga y configuración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98a8defc-178c-4e58-aba1-06023502b2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando dataset NG-IIoTset...\n",
      "Dataset cargado: 18,982,712 filas y 45 columnas\n",
      "Memoria original: 33423.62 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Cargando dataset NG-IIoTset...\")\n",
    "df = pd.read_csv(\"../data/NG-IIoTset.csv\")\n",
    "print(f\"Dataset cargado: {df.shape[0]:,} filas y {df.shape[1]} columnas\")\n",
    "\n",
    "# Guardar copia para comparaciones\n",
    "df_original = df.copy()\n",
    "original_memory = df_original.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "print(f\"Memoria original: {original_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a0ea2e-5738-4f8f-bebe-887d909d410f",
   "metadata": {},
   "source": [
    "### Transformacionales especifica datetime e ipaddr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1f28ff-6892-4e50-97d4-05e7d34a6f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRANSFORMACIONES ESPECÍFICAS ===\n",
      "Transformando direcciones IP con biblioteca ipaddress...\n",
      "  Procesando id.orig_h...\n",
      "    Ejemplo: ['192.168.5.47', '192.168.5.47', '192.168.5.47'] → [3232236847, 3232236847, 3232236847]\n",
      "    Rango final: 0 - 4294967292\n",
      "  Procesando id.resp_h...\n",
      "    Ejemplo: ['192.168.5.46', '192.168.5.46', '192.168.5.46'] → [3232236846, 3232236846, 3232236846]\n",
      "    Rango final: 1 - 4294967295\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TRANSFORMACIONES ESPECÍFICAS ===\")\n",
    "\n",
    "# 2.2 Transformar IPs con ipaddress (sin crear nuevas columnas)\n",
    "print(\"Transformando direcciones IP con biblioteca ipaddress...\")\n",
    "\n",
    "def transform_ip_to_numeric(ip):\n",
    "    \"\"\"Convierte IP a valor numérico usando ipaddress\"\"\"\n",
    "    try:\n",
    "        if pd.isna(ip) or ip == 'unknown' or ip == '':\n",
    "            return 0\n",
    "        ip_obj = ipaddress.ip_address(str(ip))\n",
    "        if isinstance(ip_obj, ipaddress.IPv6Address):\n",
    "            # Para IPv6, usar los últimos 32 bits\n",
    "            return int(ip_obj) & 0xFFFFFFFF\n",
    "        else:\n",
    "            # Para IPv4, conversión directa\n",
    "            return int(ip_obj)\n",
    "    except (ipaddress.AddressValueError, ValueError):\n",
    "        return 0\n",
    "\n",
    "# Transformar las columnas IP\n",
    "ip_columns = ['id.orig_h', 'id.resp_h']\n",
    "for col in ip_columns:\n",
    "    if col in df.columns:\n",
    "        print(f\"  Procesando {col}...\")\n",
    "        original_sample = df[col].head(3).tolist()\n",
    "        df[col] = df[col].apply(transform_ip_to_numeric)\n",
    "        transformed_sample = df[col].head(3).tolist()\n",
    "        print(f\"    Ejemplo: {original_sample} → {transformed_sample}\")\n",
    "        print(f\"    Rango final: {df[col].min()} - {df[col].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ddaed-5a14-430d-9aea-8b706ac0633d",
   "metadata": {},
   "source": [
    "## Optimización de tipos de datos para NG-IIoTset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af6f09f6-e7ab-4c61-8050-d1bbe53c15ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OPTIMIZACIÓN DE TIPOS DE DATOS ===\n",
      "✓ 17 columnas convertidas a categóricas\n",
      "Memoria optimizada: 7853.86 MB\n",
      "Reducción de memoria: 76.50%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== OPTIMIZACIÓN DE TIPOS DE DATOS ===\")\n",
    "\n",
    "def optimize_datatypes(df):\n",
    "    \"\"\"Optimiza los tipos de datos del NG-IIoTset\"\"\"\n",
    "    df_opt = df.copy()\n",
    "    \n",
    "    # Columnas categóricas\n",
    "    categorical_cols = [\n",
    "        'proto', 'service', 'conn_state', 'qtype_name', 'rcode_name', \n",
    "        'connect_status', 'client_id', 'topic', 'func', 'pdu_type', \n",
    "        'exception', 'method', 'host', 'source', 'mime_type', 'name', 'typeAttack'\n",
    "    ]\n",
    "    for col in categorical_cols:\n",
    "        if col in df_opt.columns:\n",
    "            df_opt[col] = df_opt[col].astype('category')\n",
    "    print(f\"✓ {len([c for c in categorical_cols if c in df_opt.columns])} columnas convertidas a categóricas\")\n",
    "    \n",
    "    # Columnas numéricas categóricas\n",
    "    numeric_categorical = ['ip_proto', 'rcode']\n",
    "    for col in numeric_categorical:\n",
    "        if col in df_opt.columns:\n",
    "            df_opt[col] = df_opt[col].astype('category')\n",
    "    \n",
    "    # Optimización de enteros y flotantes\n",
    "    # Puertos\n",
    "    if 'id.resp_p' in df_opt.columns:\n",
    "        df_opt['id.resp_p'] = df_opt['id.resp_p'].astype('uint16')\n",
    "    \n",
    "    # IPs ya transformadas a uint64\n",
    "    for col in ['id.orig_h', 'id.resp_h']:\n",
    "        if col in df_opt.columns:\n",
    "            df_opt[col] = df_opt[col].astype('uint64')\n",
    "    \n",
    "    # Timestamp ya transformado\n",
    "    if 'ts' in df_opt.columns:\n",
    "        df_opt['ts'] = df_opt['ts'].astype('float64')\n",
    "    \n",
    "    # Contadores de paquetes\n",
    "    packet_cols = ['orig_pkts', 'resp_pkts']\n",
    "    for col in packet_cols:\n",
    "        if col in df_opt.columns:\n",
    "            df_opt[col] = df_opt[col].astype('uint32')\n",
    "    \n",
    "    # Bytes\n",
    "    byte_cols = ['orig_bytes', 'resp_bytes', 'request_body_len', \n",
    "                 'response_body_len', 'seen_bytes', 'total_bytes']\n",
    "    for col in byte_cols:\n",
    "        if col in df_opt.columns:\n",
    "            max_val = df_opt[col].max()\n",
    "            if max_val <= 65535:\n",
    "                df_opt[col] = df_opt[col].astype('uint16')\n",
    "            elif max_val <= 4294967295:\n",
    "                df_opt[col] = df_opt[col].astype('uint32')\n",
    "            else:\n",
    "                df_opt[col] = df_opt[col].astype('uint64')\n",
    "    \n",
    "    # Duración y código de estado\n",
    "    if 'duration' in df_opt.columns:\n",
    "        df_opt['duration'] = df_opt['duration'].astype('float32')\n",
    "    if 'status_code' in df_opt.columns:\n",
    "        df_opt['status_code'] = df_opt['status_code'].astype('uint16')\n",
    "    \n",
    "    # Columnas con un solo valor\n",
    "    single_value_cols = ['filename', 'md5', 'sha1', 'sha256']\n",
    "    for col in single_value_cols:\n",
    "        if col in df_opt.columns and df_opt[col].nunique() <= 1:\n",
    "            df_opt[col] = df_opt[col].astype('uint8')\n",
    "    \n",
    "    # Texto con pocos valores únicos\n",
    "    text_categorical = ['query', 'answers']\n",
    "    for col in text_categorical:\n",
    "        if col in df_opt.columns:\n",
    "            df_opt[col] = df_opt[col].astype('category')\n",
    "    \n",
    "    # Texto con muchos valores únicos\n",
    "    text_large = ['payload', 'uri', 'user_agent']\n",
    "    for col in text_large:\n",
    "        if col in df_opt.columns:\n",
    "            df_opt[col] = df_opt[col].astype(pd.StringDtype())\n",
    "    \n",
    "    # Identificadores\n",
    "    id_cols = ['uid', 'fuid']\n",
    "    for col in id_cols:\n",
    "        if col in df_opt.columns:\n",
    "            df_opt[col] = df_opt[col].astype(pd.StringDtype())\n",
    "    \n",
    "    return df_opt\n",
    "\n",
    "# Aplicar optimización\n",
    "df = optimize_datatypes(df)\n",
    "\n",
    "# Mostrar mejora en memoria\n",
    "optimized_memory = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "memory_reduction = (1 - optimized_memory/original_memory) * 100\n",
    "print(f\"Memoria optimizada: {optimized_memory:.2f} MB\")\n",
    "print(f\"Reducción de memoria: {memory_reduction:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ede9d-259e-49af-90ff-51ce2deba128",
   "metadata": {},
   "source": [
    "## Limpieza de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75133785-eba4-4f4a-990b-b9efccaa0c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LIMPIEZA DE DATOS ===\n",
      "Filas antes de eliminar duplicados: 18,982,712\n",
      "Duplicados encontrados: 11,294,503\n",
      "Filas después de eliminar duplicados: 7,688,209\n",
      "Columnas eliminadas: ['fuid', 'uid', 'ts']\n",
      "Dimensiones finales después de limpieza: (7688209, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== LIMPIEZA DE DATOS ===\")\n",
    "\n",
    "# Eliminar duplicados\n",
    "print(f\"Filas antes de eliminar duplicados: {len(df):,}\")\n",
    "duplicate_count = df.duplicated().sum()\n",
    "print(f\"Duplicados encontrados: {duplicate_count:,}\")\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Filas después de eliminar duplicados: {len(df):,}\")\n",
    "\n",
    "# Manejar valores infinitos\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Estrategia para valores nulos (comentar/descomentar según necesidad)\n",
    "null_counts = df.isnull().sum()\n",
    "#print(f\"Valores nulos: {null_counts}\")\n",
    "cols_with_nulls = null_counts[null_counts > 0]\n",
    "if not cols_with_nulls.empty:\n",
    "    #print(\"Columnas con valores nulos:\")\n",
    "    for col, count in cols_with_nulls.items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        #print(f\"  {col}: {count:,} ({percentage:.2f}%)\")\n",
    "    \n",
    "    df = df.drop(cols_with_nulls)\n",
    "\n",
    "\n",
    "# Eliminar columnas irrelevantes para ML\n",
    "columns_to_drop = ['fuid','uid', 'ts']\n",
    "existing_cols_to_drop = [col for col in columns_to_drop if col in df.columns]\n",
    "if existing_cols_to_drop:\n",
    "    df = df.drop(columns=existing_cols_to_drop)\n",
    "    print(f\"Columnas eliminadas: {existing_cols_to_drop}\")\n",
    "\n",
    "print(f\"Dimensiones finales después de limpieza: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3936b26-5ff0-4cee-a8f4-b55dae2d30a8",
   "metadata": {},
   "source": [
    "## Balanceo de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f2c2d1-2d00-45ae-8c6b-5a76619942a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BALANCEO DE CLASES ===\n",
      "Distribución original de clases:\n",
      "  ddos_udp: 3,100,000 (40.32%)\n",
      "  ddos_icmp: 2,407,982 (31.32%)\n",
      "  ddos_tcp_syn: 1,176,730 (15.31%)\n",
      "  normal: 685,517 (8.92%)\n",
      "  vuln_scan: 149,963 (1.95%)\n",
      "  password: 96,085 (1.25%)\n",
      "  ddos_http: 43,389 (0.56%)\n",
      "  port_scan: 11,145 (0.14%)\n",
      "  upload: 7,636 (0.10%)\n",
      "  sql_injection: 4,433 (0.06%)\n",
      "  xss: 2,094 (0.03%)\n",
      "  backdoor: 1,375 (0.02%)\n",
      "  ransomware: 1,369 (0.02%)\n",
      "  mitm_arp_dns: 258 (0.00%)\n",
      "  os_fingerprint: 233 (0.00%)\n",
      "Clase minoritaria: os_fingerprint con 233 registros\n",
      "  normal: 685,517 → 5,656 (undersampled)\n",
      "  backdoor: 1,375 (kept all)\n",
      "  ddos_http: 43,389 → 2,458 (undersampled)\n",
      "  ddos_icmp: 2,407,982 → 3,279 (undersampled)\n",
      "  ddos_tcp_syn: 1,176,730 → 2,385 (undersampled)\n",
      "  ddos_udp: 3,100,000 → 3,374 (undersampled)\n",
      "  os_fingerprint: 233 (kept all)\n",
      "  mitm_arp_dns: 258 (kept all)\n",
      "  password: 96,085 → 2,325 (undersampled)\n",
      "  port_scan: 11,145 → 2,344 (undersampled)\n",
      "  ransomware: 1,369 (kept all)\n",
      "  sql_injection: 4,433 → 2,400 (undersampled)\n",
      "  upload: 7,636 → 2,390 (undersampled)\n",
      "  vuln_scan: 149,963 → 2,345 (undersampled)\n",
      "  xss: 2,094 (kept all)\n",
      "\n",
      "Dataset balanceado: 34,285 registros\n",
      "  normal: 5,656 (16.50%)\n",
      "  ddos_udp: 3,374 (9.84%)\n",
      "  ddos_icmp: 3,279 (9.56%)\n",
      "  ddos_http: 2,458 (7.17%)\n",
      "  sql_injection: 2,400 (7.00%)\n",
      "  upload: 2,390 (6.97%)\n",
      "  ddos_tcp_syn: 2,385 (6.96%)\n",
      "  vuln_scan: 2,345 (6.84%)\n",
      "  port_scan: 2,344 (6.84%)\n",
      "  password: 2,325 (6.78%)\n",
      "  xss: 2,094 (6.11%)\n",
      "  backdoor: 1,375 (4.01%)\n",
      "  ransomware: 1,369 (3.99%)\n",
      "  mitm_arp_dns: 258 (0.75%)\n",
      "  os_fingerprint: 233 (0.68%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== BALANCEO DE CLASES ===\")\n",
    "\n",
    "# Mostrar distribución original\n",
    "print(\"Distribución original de clases:\")\n",
    "original_distribution = df['typeAttack'].value_counts()\n",
    "for attack_type, count in original_distribution.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  {attack_type}: {count:,} ({percentage:.2f}%)\")\n",
    "\n",
    "# Distribución objetivo basada en el paper de referencia\n",
    "target_distribution = {\n",
    "    'normal': 24301, 'backdoor': 10195, 'ddos_http': 10561, 'ddos_icmp': 14090,\n",
    "    'ddos_tcp_syn': 10247, 'ddos_udp': 14498, 'os_fingerprint': 1001,\n",
    "    'mitm_arp_dns': 1214, 'password': 9989, 'port_scan': 10071,\n",
    "    'ransomware': 10925, 'sql_injection': 10311, 'upload': 10269,\n",
    "    'vuln_scan': 10076, 'xss': 10052\n",
    "}\n",
    "\n",
    "def create_proportional_sample(df, target_dist):\n",
    "    \"\"\"Crea un muestreo proporcional basado en la clase minoritaria actual\"\"\"\n",
    "    \n",
    "    # Encontrar la clase minoritaria actual\n",
    "    current_counts = {attack_type: len(df[df['typeAttack'] == attack_type]) \n",
    "                     for attack_type in target_dist.keys()}\n",
    "    \n",
    "    min_class_name = min(current_counts.items(), key=lambda x: x[1])[0]\n",
    "    min_class_count = current_counts[min_class_name]\n",
    "    \n",
    "    print(f\"Clase minoritaria: {min_class_name} con {min_class_count:,} registros\")\n",
    "    \n",
    "    # Calcular proporciones objetivo\n",
    "    min_target_count = target_dist[min_class_name]\n",
    "    target_proportions = {attack_type: count / min_target_count \n",
    "                         for attack_type, count in target_dist.items()}\n",
    "    \n",
    "    # Calcular muestras a tomar\n",
    "    samples_to_take = {}\n",
    "    for attack_type, proportion in target_proportions.items():\n",
    "        target_samples = int(min_class_count * proportion)\n",
    "        available_samples = current_counts[attack_type]\n",
    "        samples_to_take[attack_type] = min(target_samples, available_samples)\n",
    "    \n",
    "    # Crear dataset balanceado\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for attack_type, n_samples in samples_to_take.items():\n",
    "        class_df = df[df['typeAttack'] == attack_type]\n",
    "        if n_samples < len(class_df):\n",
    "            sampled_df = class_df.sample(n=n_samples, random_state=42)\n",
    "            print(f\"  {attack_type}: {len(class_df):,} → {n_samples:,} (undersampled)\")\n",
    "        else:\n",
    "            sampled_df = class_df\n",
    "            print(f\"  {attack_type}: {len(class_df):,} (kept all)\")\n",
    "        \n",
    "        balanced_df = pd.concat([balanced_df, sampled_df])\n",
    "    \n",
    "    return balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Aplicar balanceo\n",
    "df_balanced = create_proportional_sample(df, target_distribution)\n",
    "\n",
    "# Mostrar distribución final\n",
    "print(f\"\\nDataset balanceado: {len(df_balanced):,} registros\")\n",
    "final_distribution = df_balanced['typeAttack'].value_counts()\n",
    "for attack_type, count in final_distribution.items():\n",
    "    percentage = (count / len(df_balanced)) * 100\n",
    "    print(f\"  {attack_type}: {count:,} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6e03b-e072-4eaa-bd21-e41c62421c25",
   "metadata": {},
   "source": [
    "## Codificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "074c4750-64f1-480e-aa7d-9a1af26c2c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CODIFICACIÓN ===\n",
      "Columnas categóricas a codificar: 23\n",
      "  ✓ proto\n",
      "  ✓ service\n",
      "  ✓ conn_state\n",
      "  ✓ ip_proto\n",
      "  ✓ query\n",
      "  ✓ answers\n",
      "  ✓ qtype_name\n",
      "  ✓ rcode\n",
      "  ✓ rcode_name\n",
      "  ✓ connect_status\n",
      "  ✓ client_id\n",
      "  ✓ topic\n",
      "  ✓ payload\n",
      "  ✓ func\n",
      "  ✓ pdu_type\n",
      "  ✓ exception\n",
      "  ✓ method\n",
      "  ✓ uri\n",
      "  ✓ user_agent\n",
      "  ✓ host\n",
      "  ✓ source\n",
      "  ✓ mime_type\n",
      "  ✓ name\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== CODIFICACIÓN ===\")\n",
    "\n",
    "# Identificar columnas categóricas (excluyendo variable objetivo)\n",
    "categorical_cols = df_balanced.select_dtypes(include=['object', 'category', 'string']).columns\n",
    "categorical_cols = [col for col in categorical_cols if col != 'typeAttack']\n",
    "\n",
    "print(f\"Columnas categóricas a codificar: {len(categorical_cols)}\")\n",
    "\n",
    "# Aplicar Label Encoding\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in df_balanced.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Manejar valores nulos\n",
    "        if df_balanced[col].isna().any():\n",
    "            df_balanced[col] = df_balanced[col].fillna('MISSING')\n",
    "        \n",
    "        df_balanced[col] = le.fit_transform(df_balanced[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"  ✓ {col}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dea00c6-16b9-4b84-a6ae-3fdbdb8822ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34285, 42)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc93fbee-1a26-4228-90a6-50ff4ab8046d",
   "metadata": {},
   "source": [
    "## Analisis de importancia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf4b416a-6e0e-4c87-a570-ce28c8b8f48a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANÁLISIS DE SELECCIÓN DE CARACTERÍSTICAS ===\n",
      "Método: Información Mutua (Mutual Information)\n",
      "Justificación: Detecta relaciones lineales y no lineales entre variables y objetivos\n",
      "Variables disponibles para análisis: 40\n",
      "\n",
      "  Calculando Información Mutua para clasificación binaria...\n",
      "\n",
      "  Calculando Información Mutua para clasificación multiclase...\n",
      "\n",
      "  Combinando resultados de ambas clasificaciones...\n",
      "\n",
      "=== TOP 20 CARACTERÍSTICAS SELECCIONADAS ===\n",
      "Criterio: Mayor Información Mutua promedio (Binaria + Multiclase)\n",
      "\n",
      "   1. duration\n",
      "   2. orig_bytes\n",
      "   3. id.orig_h\n",
      "   4. id.resp_p\n",
      "   5. resp_bytes\n",
      "   6. resp_pkts\n",
      "   7. orig_pkts\n",
      "   8. service\n",
      "   9. conn_state\n",
      "  10. id.resp_h\n",
      "  11. uri\n",
      "  12. user_agent\n",
      "  13. connect_status\n",
      "  14. payload\n",
      "  15. client_id\n",
      "  16. topic\n",
      "  17. ip_proto\n",
      "  18. proto\n",
      "  19. total_bytes\n",
      "  20. method\n",
      "\n",
      "✓ Dataset reducido creado con 20 características para análisis de correlación\n"
     ]
    }
   ],
   "source": [
    "# 7.1 Análisis de Selección de Características basado en Información Mutua\n",
    "print(\"\\n=== ANÁLISIS DE SELECCIÓN DE CARACTERÍSTICAS ===\")\n",
    "print(\"Método: Información Mutua (Mutual Information)\")\n",
    "print(\"Justificación: Detecta relaciones lineales y no lineales entre variables y objetivos\")\n",
    "\n",
    "# Preparar datos para análisis (solo variables numéricas)\n",
    "X = df_balanced.select_dtypes(include=[np.number]).copy()\n",
    "y_multiclass = df_balanced['typeAttack'].copy()\n",
    "\n",
    "# Codificar variable objetivo multiclase para análisis\n",
    "le_target = LabelEncoder()\n",
    "y_multiclass_encoded = le_target.fit_transform(y_multiclass)\n",
    "y_binary = df_balanced['isAttack'].copy()\n",
    "\n",
    "# Eliminar variables objetivo del conjunto de features\n",
    "feature_cols = [col for col in X.columns if col not in ['isAttack']]\n",
    "X_features = X[feature_cols].copy()\n",
    "\n",
    "print(f\"Variables disponibles para análisis: {len(X_features.columns)}\")\n",
    "\n",
    "# 7.1.1 Análisis de Información Mutua para Clasificación Binaria\n",
    "print(\"\\n  Calculando Información Mutua para clasificación binaria...\")\n",
    "mi_binary = mutual_info_classif(X_features, y_binary, random_state=42)\n",
    "mi_binary_df = pd.DataFrame({\n",
    "    'feature': X_features.columns,\n",
    "    'mi_binary': mi_binary\n",
    "}).sort_values('mi_binary', ascending=False)\n",
    "\n",
    "#print(\"Top 20 características para clasificación binaria (Normal vs Ataque):\")\n",
    "#print(mi_binary_df.head(20))\n",
    "\n",
    "# 7.1.2 Análisis de Información Mutua para Clasificación Multiclase\n",
    "print(\"\\n  Calculando Información Mutua para clasificación multiclase...\")\n",
    "mi_multiclass = mutual_info_classif(X_features, y_multiclass_encoded, random_state=42)\n",
    "mi_multiclass_df = pd.DataFrame({\n",
    "    'feature': X_features.columns,\n",
    "    'mi_multiclass': mi_multiclass\n",
    "}).sort_values('mi_multiclass', ascending=False)\n",
    "\n",
    "#print(\"Top 20 características para clasificación multiclase (20 tipos de amenazas):\")\n",
    "#print(mi_multiclass_df.head(20))\n",
    "\n",
    "# 7.1.3 Combinación de Información Mutua\n",
    "print(\"\\n  Combinando resultados de ambas clasificaciones...\")\n",
    "mi_analysis = mi_binary_df.merge(mi_multiclass_df, on='feature')\n",
    "mi_analysis['avg_mi'] = (mi_analysis['mi_binary'] + mi_analysis['mi_multiclass']) / 2\n",
    "mi_analysis = mi_analysis.sort_values('avg_mi', ascending=False)\n",
    "\n",
    "# 7.1.5 Selección final de características\n",
    "top_features_count = 20\n",
    "top_features = mi_analysis.head(top_features_count)['feature'].tolist()\n",
    "\n",
    "print(f\"\\n=== TOP {top_features_count} CARACTERÍSTICAS SELECCIONADAS ===\")\n",
    "print(\"Criterio: Mayor Información Mutua promedio (Binaria + Multiclase)\")\n",
    "print()\n",
    "for i, feat in enumerate(top_features, 1):\n",
    "    row = mi_analysis[mi_analysis['feature'] == feat].iloc[0]\n",
    "    print(f\"  {i:2d}. {feat}\")\n",
    "\n",
    "# Crear dataset con top características para análisis de correlación\n",
    "X_top_features = X_features[top_features].copy()\n",
    "print(f\"\\n✓ Dataset reducido creado con {len(top_features)} características para análisis de correlación\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87614a8c-56ad-4dd7-a07e-cb244fdd5a00",
   "metadata": {},
   "source": [
    "### Correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3467d0d-fb69-4599-902b-a055031ce5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ANÁLISIS DE CORRELACIÓN ===\n",
      "Calculando matriz de correlación para top características...\n",
      "\n",
      "Pares de variables con correlación > 0.9: 4\n",
      "Pares altamente correlacionados:\n",
      "  orig_bytes ↔ resp_pkts: 0.980\n",
      "  orig_bytes ↔ orig_pkts: 0.983\n",
      "  resp_pkts ↔ orig_pkts: 0.999\n",
      "  connect_status ↔ client_id: 1.000\n",
      "  → Eliminar resp_pkts (corr: 0.980, score: 0.737240)\n",
      "  → Eliminar orig_pkts (corr: 0.983, score: 0.716456)\n",
      "  → Eliminar client_id (corr: 1.000, score: 0.420146)\n",
      "\n",
      "Características finales después de eliminar correlaciones:\n",
      "  Características iniciales: 40\n",
      "  Top características seleccionadas: 20\n",
      "  Eliminadas por alta correlación: 3\n",
      "  Características finales: 17\n",
      "\n",
      "Lista de características finales:\n",
      "   1. duration: 0.939362\n",
      "   2. orig_bytes: 0.869379\n",
      "   3. id.orig_h: 0.864469\n",
      "   4. id.resp_p: 0.842987\n",
      "   5. resp_bytes: 0.812425\n",
      "   6. service: 0.612794\n",
      "   7. conn_state: 0.571320\n",
      "   8. id.resp_h: 0.549681\n",
      "   9. uri: 0.437623\n",
      "  10. user_agent: 0.435129\n",
      "  11. connect_status: 0.425289\n",
      "  12. payload: 0.425111\n",
      "  13. topic: 0.419869\n",
      "  14. ip_proto: 0.362774\n",
      "  15. proto: 0.305949\n",
      "  16. total_bytes: 0.265340\n",
      "  17. method: 0.264705\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== ANÁLISIS DE CORRELACIÓN ===\")\n",
    "print(\"Calculando matriz de correlación para top características...\")\n",
    "correlation_matrix = X_top_features.corr().abs()\n",
    "\n",
    "# Encontrar pares de variables altamente correlacionadas (> 0.9)\n",
    "high_correlation_threshold = 0.9\n",
    "high_corr_pairs = []\n",
    "\n",
    "# Solo analizar la matriz triangular superior para evitar duplicados\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        if corr_value > high_correlation_threshold:\n",
    "            col1 = correlation_matrix.columns[i]\n",
    "            col2 = correlation_matrix.columns[j]\n",
    "            high_corr_pairs.append((col1, col2, corr_value))\n",
    "\n",
    "print(f\"\\nPares de variables con correlación > {high_correlation_threshold}: {len(high_corr_pairs)}\")\n",
    "if high_corr_pairs:\n",
    "    print(\"Pares altamente correlacionados:\")\n",
    "    for col1, col2, corr_val in high_corr_pairs:\n",
    "        print(f\"  {col1} ↔ {col2}: {corr_val:.3f}\")\n",
    "else:\n",
    "    print(\"No se encontraron pares altamente correlacionados.\")\n",
    "\n",
    "# Seleccionar variables a eliminar (mantener la que tenga mayor puntuación combinada)\n",
    "features_to_remove_corr = []\n",
    "for col1, col2, corr_val in high_corr_pairs:\n",
    "    score1 = mi_analysis[mi_analysis['feature'] == col1]['avg_mi'].iloc[0]\n",
    "    score2 = mi_analysis[mi_analysis['feature'] == col2]['avg_mi'].iloc[0]\n",
    "    \n",
    "    # Eliminar la que tenga menor puntuación combinada\n",
    "    feature_to_remove = col1 if score1 < score2 else col2\n",
    "    if feature_to_remove not in features_to_remove_corr:\n",
    "        features_to_remove_corr.append(feature_to_remove)\n",
    "        print(f\"  → Eliminar {feature_to_remove} (corr: {corr_val:.3f}, score: {min(score1, score2):.6f})\")\n",
    "\n",
    "# Crear lista final de características\n",
    "final_features = [feat for feat in top_features if feat not in features_to_remove_corr]\n",
    "print(f\"\\nCaracterísticas finales después de eliminar correlaciones:\")\n",
    "print(f\"  Características iniciales: {len(X_features.columns)}\")\n",
    "print(f\"  Top características seleccionadas: {len(top_features)}\")\n",
    "print(f\"  Eliminadas por alta correlación: {len(features_to_remove_corr)}\")\n",
    "print(f\"  Características finales: {len(final_features)}\")\n",
    "\n",
    "print(f\"\\nLista de características finales:\")\n",
    "for i, feat in enumerate(final_features, 1):\n",
    "    score = mi_analysis[mi_analysis['feature'] == feat]['avg_mi'].iloc[0]\n",
    "    print(f\"  {i:2d}. {feat}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7f4921d-cc6e-47d7-a332-7ad6ab023dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen de reducción de características:\n",
      "  Variables originales: 40\n",
      "  Variables en top ranking: 20\n",
      "  Variables eliminadas por correlación: 3\n",
      "  Variables finales: 17\n",
      "\n",
      "Dataset final: (34285, 19)\n",
      "Variables predictoras: 17\n",
      "Variables objetivo: 2 (typeAttack, isAttack)\n"
     ]
    }
   ],
   "source": [
    "# 7.3 Aplicar reducción de características\n",
    "print(f\"\\nResumen de reducción de características:\")\n",
    "print(f\"  Variables originales: {len(X_features.columns)}\")\n",
    "print(f\"  Variables en top ranking: {len(top_features)}\")\n",
    "print(f\"  Variables eliminadas por correlación: {len(features_to_remove_corr)}\")\n",
    "print(f\"  Variables finales: {len(final_features)}\")\n",
    "\n",
    "# Aplicar reducción al dataset balanceado usando las características finales\n",
    "df_reduced = df_balanced.copy()\n",
    "\n",
    "# Mantener solo las características finales + variables objetivo\n",
    "columns_to_keep = final_features + ['typeAttack', 'isAttack']\n",
    "existing_columns_to_keep = [col for col in columns_to_keep if col in df_reduced.columns]\n",
    "\n",
    "# Crear dataset final con características seleccionadas\n",
    "df_reduced = df_reduced[existing_columns_to_keep]\n",
    "\n",
    "print(f\"\\nDataset final: {df_reduced.shape}\")\n",
    "print(f\"Variables predictoras: {len(final_features)}\")\n",
    "print(f\"Variables objetivo: 2 (typeAttack, isAttack)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98c838-23a0-432d-b011-481c4f8ab150",
   "metadata": {},
   "source": [
    "## Verificaciones finales y guardado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acd5f540-4d8b-4701-9af8-078e3e9defb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VERIFICACIONES FINALES ===\n",
      "Valores nulos en variables predictoras: 0\n",
      "Distribución binaria - Normal: 5,656, Ataques: 28,629\n",
      "Distribución final de clases: 15 clases\n",
      "\n",
      "=== GUARDADO ===\n",
      "✓ CSV guardado: ML_NG-IIoTset.csv\n",
      "✓ PKL guardado: ML_NG-IIoTset.pkl\n",
      "✓ encoders guardados\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== VERIFICACIONES FINALES ===\")\n",
    "\n",
    "# Verificar que no hay valores nulos en variables predictoras\n",
    "predictor_cols = [col for col in df_reduced.columns if col not in ['typeAttack', 'isAttack']]\n",
    "null_check = df_reduced[predictor_cols].isnull().sum().sum()\n",
    "print(f\"Valores nulos en variables predictoras: {null_check}\")\n",
    "\n",
    "# Verificar distribución binaria\n",
    "binary_dist = df_reduced['isAttack'].value_counts()\n",
    "print(f\"Distribución binaria - Normal: {binary_dist.get(0, 0):,}, Ataques: {binary_dist.get(1, 0):,}\")\n",
    "\n",
    "# Verificar distribución de clases\n",
    "final_class_dist = df_reduced['typeAttack'].value_counts()\n",
    "print(f\"Distribución final de clases: {len(final_class_dist)} clases\")\n",
    "\n",
    "print(\"\\n=== GUARDADO ===\")\n",
    "\n",
    "# Guardar dataset procesado\n",
    "df_reduced.to_csv('../data/ML_NG-IIoTset.csv', index=False)\n",
    "print(\"✓ CSV guardado: ML_NG-IIoTset.csv\")\n",
    "\n",
    "# Guardar en formato pickle\n",
    "with open('../data/ML_NG-IIoTset.pkl', 'wb') as f:\n",
    "    pickle.dump(df_reduced, f)\n",
    "print(\"✓ PKL guardado: ML_NG-IIoTset.pkl\")\n",
    "\n",
    "# Guardar  encoders\n",
    "joblib.dump(label_encoders, '../data/label_encoders.pkl')\n",
    "joblib.dump(le_target, '../data/target_encoder.pkl')\n",
    "print(\"✓ encoders guardados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9158c6f9-59ae-4f63-8841-a5f21f0a8d2d",
   "metadata": {},
   "source": [
    "## Resumen final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "605d1713-bb3c-4a9c-ae9e-0bb9b23ac172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RESUMEN DEL PREPROCESAMIENTO\n",
      "============================================================\n",
      "Dataset original: 18,982,712 × 45\n",
      "Duplicados eliminados: 11,294,503\n",
      "Dataset balanceado: 34,285 × 42\n",
      "Top características seleccionadas: 20\n",
      "Variables eliminadas por correlación: 3\n",
      "Dataset final: 34,285 × 19\n",
      "Variables predictoras finales: 17\n",
      "Reducción de registros: 99.82%\n",
      "Reducción de memoria: 76.50%\n",
      "\n",
      "Transformaciones aplicadas:\n",
      "  ✓ Timestamp transformado con datetime\n",
      "  ✓ IPs transformadas con ipaddress\n",
      "  ✓ Tipos de datos optimizados\n",
      "  ✓ Duplicados eliminados\n",
      "  ✓ Clases balanceadas proporcionalmente\n",
      "  ✓ Variables categóricas codificadas\n",
      "  ✓ Análisis de importancia realizado\n",
      "  ✓ Variables correlacionadas eliminadas\n",
      "\n",
      "✅ Dataset listo para entrenamiento de modelos ML\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESUMEN DEL PREPROCESAMIENTO\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset original: {df_original.shape[0]:,} × {df_original.shape[1]}\")\n",
    "print(f\"Duplicados eliminados: {duplicate_count:,}\")\n",
    "print(f\"Dataset balanceado: {len(df_balanced):,} × {len(df_balanced.columns)}\")\n",
    "print(f\"Top características seleccionadas: {len(top_features)}\")  \n",
    "print(f\"Variables eliminadas por correlación: {len(features_to_remove_corr)}\")   \n",
    "print(f\"Dataset final: {df_reduced.shape[0]:,} × {df_reduced.shape[1]}\")\n",
    "print(f\"Variables predictoras finales: {len(predictor_cols)}\")\n",
    "print(f\"Reducción de registros: {(1 - len(df_reduced)/len(df_original))*100:.2f}%\")\n",
    "print(f\"Reducción de memoria: {memory_reduction:.2f}%\")\n",
    "\n",
    "print(\"\\nTransformaciones aplicadas:\")\n",
    "print(\"  ✓ Timestamp transformado con datetime\")\n",
    "print(\"  ✓ IPs transformadas con ipaddress\")\n",
    "print(\"  ✓ Tipos de datos optimizados\")\n",
    "print(\"  ✓ Duplicados eliminados\")\n",
    "print(\"  ✓ Clases balanceadas proporcionalmente\")\n",
    "print(\"  ✓ Variables categóricas codificadas\")\n",
    "print(\"  ✓ Análisis de importancia realizado\")\n",
    "print(\"  ✓ Variables correlacionadas eliminadas\")\n",
    "\n",
    "\n",
    "print(\"\\n✅ Dataset listo para entrenamiento de modelos ML\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d027c54f-0876-4cfe-9c90-96fd82876b40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
